import theano
import theano.tensor as T
import lasagne as nn
from lasagne.layers import dnn
import nn_eyes
import data
import buffering
import tmp_dnn
import numpy as np
import customLayers
# import nn_plankton

prob_Out = False
validate_every = 10
save_every = 40

num_chunks = 4000
chunk_size = 1600
batch_size = 32
im_height = 256
im_width = 256
image_size = (im_height, im_width)
color_pertube = True

# Set momentum hyper parameter to 0.9
momentum = 0.9

# Specify learning rate decay during training
learning_rate_schedule = {
    0: 0.0001,
    700: 0.00003,
    800: 0.000003,
}

augmentation_params = {

'zoom_range': (1/ 1.1, 1.1),
'rotation_range': (0, 1),
'shear_range': (-1, 1),
'translation_range': (-2, 2)
}


leakiness = 0.3
Conv2DLayer = nn.layers.dnn.Conv2DDNNLayer
MaxPool2DLayer = nn.layers.dnn.MaxPool2DDNNLayer

#Conv2DLayer = dnn.Conv2DDNNLayer
#MaxPool2DLayer = tmp_dnn.MaxPool2DDNNLayer


def build_model():
    l_in = nn.layers.InputLayer((batch_size, 3, im_height, im_width))

    l1a_c = Conv2DLayer(l_in, num_filters=8, filter_size=(3, 3), border_mode='same', W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))
    l1b_c = Conv2DLayer(l1a_c, num_filters=8, filter_size=(3, 3), border_mode='same', W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))
    l1_pool = MaxPool2DLayer(l1b_c, (2, 2), (2, 2))
    
    l2a_c = Conv2DLayer(l1_pool, num_filters=16, filter_size=(3, 3), border_mode='same', W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))
    l2b_c = Conv2DLayer(l2a_c, num_filters=16, filter_size=(3, 3), border_mode='same', W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))
    l2_pool = MaxPool2DLayer(l2b_c, (2, 2), (2, 2))

    l3a_c = Conv2DLayer(l2_pool, num_filters=32, filter_size=(3, 3), border_mode='same', W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))
    l3b_c = Conv2DLayer(l3a_c, num_filters=32, filter_size=(3, 3), border_mode='same', W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))
    l3_pool = MaxPool2DLayer(l3b_c, (2, 2), (2, 2))
    

    l6 = nn.layers.DenseLayer(nn.layers.dropout(l3_pool, p=0.5), num_units=64, W=nn.init.Orthogonal('relu'), nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness))

    l_out = nn.layers.DenseLayer(nn.layers.dropout(l6, p=0.5), num_units=1, nonlinearity=nn.nonlinearities.LeakyRectify(leakiness=leakiness), W=nn.init.Orthogonal('relu'), b=nn.init.Constant(2))

    return l_in, l_out


def build_objective(l_in, l_out):

    return nn.objectives.Objective(l_out, loss_function=customLayers.mse)


def create_train_gen():
    # Generator function that yields a new image and corresponding label when it's called
    image_gen = data.gen_images(data.paths_my_train, data.labels_my_train, shuffle=True, repeat=True)

    # Generator function that takes the image generated by image_gen and transform according to the parameters
    # set in "augmentation_params" or according to the default parameters specified inside the functions.
    def augmented_image_gen():
        for image, label in image_gen:
            #yield image, label
            yield data.augment_image(image, image_size, color_pertube, augmentation_params), label

    # Generator that generates chunk of data where the images have transformed by the function augmented_image_gen
    chunks_gen = data.gen_chunks(augmented_image_gen(), image_size=image_size, chunk_size=chunk_size, labels=True)
    return buffering.buffered_gen_threaded(chunks_gen)